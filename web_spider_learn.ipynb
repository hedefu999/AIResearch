{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbfa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请求成功\n",
      "给你看看内容类型 image/png\n",
      "我想，图片已经保存了\n"
     ]
    }
   ],
   "source": [
    "# 开始爬虫的学习\n",
    "# import urllib.request\n",
    "# req = urllib.request.Request('https://dummyimage.com/300x100&text=hello')\n",
    "# response = urllib.request.urlopen(req)\n",
    "\n",
    "# urllib访问不了https服务，不研究怎么改了\n",
    "# 直接使用Requests库\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "resp=requests.get('https://dummyimage.com/300x100&text=宁好')\n",
    "if resp.status_code == 200:\n",
    "    print('请求成功')\n",
    "    print(f'给你看看内容类型 {resp.headers['content-type']}')\n",
    "    file_path = os.path.expanduser('~/dummy_image.png')\n",
    "    with open(file_path, 'wb') as localFile:\n",
    "        localFile.write(resp.content)\n",
    "    print('我想，图片已经保存了')\n",
    "else:\n",
    "    print('请求失败')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30066478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title = 肖申克的救赎, rating = 9.7\n",
      "title = 霸王别姬, rating = 9.6\n",
      "title = 泰坦尼克号, rating = 9.5\n",
      "title = 阿甘正传, rating = 9.5\n",
      "title = 千与千寻, rating = 9.4\n",
      "title = 美丽人生, rating = 9.5\n",
      "title = 这个杀手不太冷, rating = 9.4\n",
      "title = 星际穿越, rating = 9.4\n",
      "title = 盗梦空间, rating = 9.4\n",
      "title = 楚门的世界, rating = 9.4\n",
      "title = 辛德勒的名单, rating = 9.5\n",
      "title = 忠犬八公的故事, rating = 9.4\n",
      "title = 海上钢琴师, rating = 9.3\n",
      "title = 三傻大闹宝莱坞, rating = 9.2\n",
      "title = 疯狂动物城, rating = 9.2\n",
      "title = 放牛班的春天, rating = 9.3\n",
      "title = 机器人总动员, rating = 9.3\n",
      "title = 无间道, rating = 9.3\n",
      "title = 控方证人, rating = 9.6\n",
      "title = 大话西游之大圣娶亲, rating = 9.2\n",
      "title = 熔炉, rating = 9.3\n",
      "title = 触不可及, rating = 9.3\n",
      "title = 教父, rating = 9.3\n",
      "title = 寻梦环游记, rating = 9.1\n",
      "title = 当幸福来敲门, rating = 9.1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "\n",
    "# 不设置这个headers直接爬豆瓣会返回418\n",
    "# headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
    "# resp = requests.get('https://movie.douban.com/top250', headers=headers)\n",
    "# print(len(resp.content))\n",
    "\n",
    "# 用浏览器下载了网页，访问本地文件来调试程序\n",
    "# 在WSL环境下，home目录读取方式仍然可用，跟mac/linux一致~\n",
    "file_path = os.path.expanduser('~/Developer/ArtificialIntelligence/web_spider_douban/豆瓣电影 Top 250.html')\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "soup = bs4.BeautifulSoup(html_content, 'html.parser')\n",
    "movie_entries = soup.find_all('div', class_='item')\n",
    "for entry in movie_entries:\n",
    "    title = entry.find('span', class_='title').text\n",
    "    rating = entry.find('span', class_='rating_num').text\n",
    "    print(f'title = {title}, rating = {rating}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfed290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取网易云音乐\n",
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "\n",
    "def get_url(url):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    return res\n",
    "def main():\n",
    "    url = input('请输入链接地址：')\n",
    "    res = get_url(url)\n",
    "    with open('res.txt', 'w') as file:\n",
    "        file.write(res.text)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "misc_path=os.path.expanduser('~/Developer/CodeRepo/AIResearch/misc/')\n",
    "# 防止python报文件夹不存在的问题\n",
    "os.makedirs(os.path.dirname(misc_path), exist_ok=True)\n",
    "\n",
    "def get_comments(song_id):\n",
    "    referer='https://music.163.com/song?id='+str(song_id)\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36',\n",
    "        'referer':referer,\n",
    "        'origin':'https://music.163.com',\n",
    "        'content-type':'application/x-www-form-urlencoded'\n",
    "    }\n",
    "    data = {\n",
    "        'params': 'uPf94D3xw0ovmP904Z3C3bQhycIp2agJCNjUikFYTFMsUr54YslAsIEG9ZVmv6tr7D9WeCMHZ8/jj3yqw2XCSv3c4iQdf3zMepMiQWiWlN1qwe4mz/9HbwyBg0djVUu+U9JOKVIAYtu9U6l/4A7ZIXeqfjaRkG8mIh59G83sPa3vH/ODj+m3WIRUQj40Aw8PwHlH008xGGwf/NB1tvAAL4w2p4LBKDkGCntevXvPF9II0aEqNfnQB+ngciWI2+0ySwv5iYtzNn96/uy6yWexJr5+QGOw4YqnllvwW+ojyx0=',\n",
    "        'encSecKey': '0f9340b2eb1540b8b8596b3c6a7d1e6f62c3b48888667605a9a25134fa3a96c05153700fc7cca9c4d30a1e20ab604f03a67d13d74e815be8e6dccd98e825d66cfd0519ff05e35aac57e6c0a9c58557ec1046a0ff6c4ee925f7c42c84c88aa02e6bbf387f9ee28073f0652f09e5f8b84f8f77a31638ba70fcac45efbe8433e73d'\n",
    "    }\n",
    "    target_url='https://music.163.com/weapi/comment/resource/comments/get?csrf_token='\n",
    "\n",
    "    res=requests.post(target_url, headers=headers, data=data)\n",
    "    filename=f'comments_{song_id}.json'\n",
    "    with open(misc_path + filename,'w') as file:\n",
    "        file.write(res.text)\n",
    "    return filename\n",
    "def get_hot_comments(song_id):\n",
    "    with open(misc_path+f'comments_{song_id}.json','r') as file:\n",
    "        jsondata = file.read()\n",
    "    comments_json = json.loads(jsondata)\n",
    "    hot_comments = comments_json['data']['hotComments']\n",
    "    with open(misc_path+f'hotcomts_{song_id}.txt', 'w') as file:\n",
    "        for each in hot_comments:\n",
    "            file.write(each['user']['nickname']+':\\n')\n",
    "            file.write(each['content']+'\\n\\n')\n",
    "def main():\n",
    "    song_id=1807799505\n",
    "    # filename=get_comments(song_id)\n",
    "    get_hot_comments(song_id)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a85ad75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(16, 20), match='jump'>\n",
      "<re.Match object; span=(5, 6), match='e'>\n",
      "None\n",
      "<re.Match object; span=(0, 4), match='fish'>\n",
      "start = 0, end = 33, span = (0, 33)\n"
     ]
    }
   ],
   "source": [
    "# 研究下python的正则表达式\n",
    "import re\n",
    "content='Little grey fox jump through over the bush'\n",
    "match_result = re.search(r'jum.',  content)\n",
    "print(match_result)\n",
    "# 匹配字母 a或b或e\n",
    "match_result = re.search(r'[abe]',  content)\n",
    "print(match_result)\n",
    "comments = '''\n",
    "    对于一些模式功能的说明\n",
    "    [iI] 可以匹配到content中的第一个字母\n",
    "    [a-z] 匹配任意小写字母\n",
    "    [1-9][0-9][0-9] 非0开头的3位数字字符串\n",
    "    a{3} 匹配 aaa\n",
    "    ab{3,5}c  ab之间有且仅有3-5个b的字符串\n",
    "    [0-255] 0～2之后还有两个5，表示匹配一个可以为 0,1,2,5 的数字，不要认为匹配0～255共256个可能的数字\n",
    "    [0-1]\\\\d\\\\d <--- 换行字符串拿来做段落注释会有麻烦\n",
    "'''\n",
    "# \\d会报错。这个多行字符串不需要赋值给某个变量\n",
    "# [0-1]{0,1}\\d\\d 可以匹配 88/188\n",
    "match_result = re.search(r'[0-1]{0,1}\\d\\d','8')\n",
    "print(match_result)\n",
    "# [0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5] 配置 0-255 的数字，支持1位、2位、3位数字，分析过程详询AI\n",
    "match_result = re.search(r'fishC*?','fishCC') # *匹配0至多个，*?非贪婪模式下就成了匹配0个了\n",
    "print(match_result)\n",
    "\n",
    "# 正则中的元字符 .^$*+?{}[]\\|()\n",
    "# * 相当于 {0,}; + 相当于 {1,}; ? 相当于 {0,1} 但更推荐使用前者，简洁而且正则表达式引擎内部有优化\n",
    "# 使用 '<.+>' 会匹配整个 <html><p>hello</p></html> ，如果只想匹配第一个<html> 就需要非贪婪模式 '<.+?>'\n",
    "# 匹配位置的字符： 零宽断言，不会匹配任何字符，只用于匹配一个位置。\\b 表示匹配一个单词的边界， \\B 表示非单词边界\n",
    "\n",
    "# 预编译正则表达式\n",
    "p = re.compile('[A-Z]')\n",
    "p.search('HelloWorld')\n",
    "p.findall('HelloWorld')\n",
    "\n",
    "# 正则匹配取子组\n",
    "result = re.search(r'(\\w+r).*\\s(\\w+r).*', 'outdoor activity for this weekend')\n",
    "result.group()\n",
    "result.group(2) # 子组序号是 1based 的\n",
    "print(f'start = {result.start()}, end = {result.end()}, span = {result.span()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则 + 爬虫\n",
    "import urllib.request as requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "def open_url(url):\n",
    "    req = requests.Request(url)\n",
    "    req.add_header('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36')\n",
    "    page = requests.urlopen(req)\n",
    "    html = page.read().decode('utf-8')\n",
    "    return html\n",
    "def get_img(html):\n",
    "    p = r'<img class=\"BDE_Image\".*?src=\"([^\"]*\\.jpg)\".*?>'\n",
    "    imglist = re.findall(p, html)\n",
    "    try:\n",
    "        os.mkdir(\"NewPics\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    os.chdir(\"NewPics\")\n",
    "    for each in imglist:\n",
    "        filename = each.split(\"/\")[-1]\n",
    "        requests.urlretrieve(each, filename, None)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = 'http://tieba.baidu.com/p/3823765471'\n",
    "    get_img(open_url(url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e15e14",
   "metadata": {},
   "source": [
    "# 关于西瓜皮的介绍\n",
    "\n",
    "## scrapy的组成\n",
    "\n",
    "- scrapy engine 爬虫工作的核心，控制数据流在系统所有组件中流动\n",
    "- scheduler 从引擎接收requests请求并将其入队\n",
    "- downloader下载器 负责获取页面数据并提供给引擎，而后提供给spider\n",
    "- spiders spider是scrapy用户编写用于分析response，并提取出item和额外跟进的URL的类\n",
    "- Item pipeline 负责处理被spider提取出的Item，典型的处理有清理、验证及持久化\n",
    "\n",
    "## 两个中间件\n",
    "\n",
    "-  下载中间件 Downloader Middlewares 是在引擎及下载器之间的特定钩子 specific hook，处理Downloader传递给引擎的Response\n",
    "-  spider中间件 spider middlewares 是在引擎及spider之间的特定钩子，处理来自下载器的Response和发送给Item Pipeline的Item和发送给调度器的Requests\n",
    "\n",
    "## 执行流程\n",
    "\n",
    "Scrapy中的数据流是由中间的执行引擎控制的，过程大致如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4244cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy实例"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
